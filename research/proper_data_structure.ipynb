{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cnnClassifier.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataStructureConfig:\n",
    "    unprocessed_data:Path\n",
    "    processed_data : Path\n",
    "    train_folder_sal : Path\n",
    "    test_folder_sal :Path\n",
    "    train_folder_cocc : Path\n",
    "    test_folder_cocc :Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH=Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH=Path(\"params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_filepath)\n",
    "        # self.params=read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def proper_dataStructure_config(self) -> DataStructureConfig:\n",
    "        config = self.config.data_structure\n",
    "        create_directories([config.processed_data,config.train_folder_sal,config.test_folder_sal,config.train_folder_cocc,config.test_folder_cocc])\n",
    "        data_structure_config = DataStructureConfig(\n",
    "            unprocessed_data=config.unprocessed_data,\n",
    "            processed_data=config.processed_data,\n",
    "            train_folder_sal =config.train_folder_sal,\n",
    "            test_folder_sal =config.test_folder_sal,\n",
    "            train_folder_cocc =config.train_folder_cocc,\n",
    "            test_folder_cocc =config.test_folder_cocc\n",
    "        )\n",
    "        return data_structure_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-14 18:08:26,534 : INFO : common : yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-11-14 18:08:26,535 : INFO : common : created directory at: artifacts]\n",
      "[2023-11-14 18:08:26,536 : INFO : common : created directory at: artifacts/processed_data]\n",
      "[2023-11-14 18:08:26,537 : INFO : common : created directory at: artifacts/processed_data/train/Salmonella]\n",
      "[2023-11-14 18:08:26,537 : INFO : common : created directory at: artifacts/processed_data/test/Salmonella]\n",
      "[2023-11-14 18:08:26,538 : INFO : common : created directory at: artifacts/processed_data/train/Coccidiosis]\n",
      "[2023-11-14 18:08:26,539 : INFO : common : created directory at: artifacts/processed_data/test/Coccidiosis]\n"
     ]
    }
   ],
   "source": [
    "config=ConfigurationManager()\n",
    "data_structure_config=config.proper_dataStructure_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "class DataIngestion:\n",
    "    def __init__(self,config:DataIngestionConfig):\n",
    "        self.config=config\n",
    "\n",
    "    def download_file(self):\n",
    "        os.environ['KAGGLE_CONFIG_DIR'] =self.config.kaggle_path\n",
    "       \n",
    "        dataset_slug = self.config.kaggle_data\n",
    "        destination_folder = self.config.local_data_file\n",
    "        data_name=destination_folder+dataset_slug.split('/')[-1]+\".zip\"\n",
    "        print(not os.path.exists(data_name))\n",
    "        if not os.path.exists(data_name):\n",
    "            # Create the command to download the dataset into the specified folder\n",
    "\n",
    "            command = f\"kaggle datasets download -d {dataset_slug} -p {destination_folder}\"\n",
    "            print(command)\n",
    "            # Run the download command\n",
    "            os.system(command)\n",
    "            logger.info(f\"{dataset_slug} downloaded ! \")\n",
    "        else:\n",
    "            logger.info(f\"The file is already exists of size : {get_size(data_name)}\")\n",
    "    def extract_zip_file(self):\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        dataset_slug = self.config.kaggle_data\n",
    "        path_dir=unzip_path+dataset_slug.split('/')[-1]+\".zip\"\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "        try:\n",
    "            with zipfile.ZipFile(path_dir, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_path)\n",
    "            logger.info(f\"file has been successfully exxtracted .\")\n",
    "        except:\n",
    "            logger.info(\"there is some error while extracting the files\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\atanu\\\\Desktop\\\\MY_WORK\\\\mlops\\\\chicken_disease_classification'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "class ProcessingData:\n",
    "    def __init__(self,config : DataStructureConfig):\n",
    "        self.config=config\n",
    "\n",
    "    def split_data(self):\n",
    "        csv_file=self.config.unprocessed_data +\"/train_data.csv\"\n",
    "        print(csv_file)\n",
    "        data_file=pd.read_csv(csv_file)\n",
    "        train,test =train_test_split(data_file,test_size=0.2)\n",
    "        return train,test\n",
    "\n",
    "    def process_data(self):\n",
    "        train,test=self.process_data()\n",
    "        images_folder=self.config.unprocessed_data+\"/Train\"\n",
    "        train_folder=self.config.processed_data+\"/train//\"\n",
    "        test_folder=self.config.processed_data+\"/test/\"\n",
    "\n",
    "\n",
    "\n",
    "        # Iterate through the each row in the train dataframe\n",
    "        for index , row in tqdm(train.iterrows()):\n",
    "            image_name=row[\"images\"]\n",
    "            class_name=row[\"label\"]\n",
    "\n",
    "            # Move the image to the corresponding class folder\n",
    "            source_image_path = os.path.join(images_folder, image_name)\n",
    "            destination_image_path = os.path.join(train_folder, class_name+\"/\"+image_name)\n",
    "            # Check if the image file exists before moving\n",
    "            if os.path.exists(source_image_path):\n",
    "                shutil.move(source_image_path, destination_image_path)\n",
    "            else:\n",
    "                print(f\"Image '{image_name}' not found in the images folder.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "csv_file = 'path/to/your/csv_file.csv'  # Replace with your CSV file path\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Path to the folder containing the images\n",
    "images_folder = 'path/to/your/images_folder/'  # Replace with your images folder path\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in data.iterrows():\n",
    "    image_name = row['image_name']\n",
    "    class_name = row['class']\n",
    "\n",
    "    # Create folders for classes if they don't exist\n",
    "    class_folder = os.path.join(images_folder, class_name)\n",
    "    os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "    # Move the image to the corresponding class folder\n",
    "    source_image_path = os.path.join(images_folder, image_name)\n",
    "    destination_image_path = os.path.join(class_folder, image_name)\n",
    "\n",
    "    # Check if the image file exists before moving\n",
    "    if os.path.exists(source_image_path):\n",
    "        shutil.move(source_image_path, destination_image_path)\n",
    "    else:\n",
    "        print(f\"Image '{image_name}' not found in the images folder.\")\n",
    "\n",
    "    print(\"Images moved to corresponding class folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProcessingData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\atanu\\Desktop\\MY_WORK\\mlops\\chicken_disease_classification\\research\\proper_data_structure.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/atanu/Desktop/MY_WORK/mlops/chicken_disease_classification/research/proper_data_structure.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m p\u001b[39m=\u001b[39mProcessingData(data_structure_config)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atanu/Desktop/MY_WORK/mlops/chicken_disease_classification/research/proper_data_structure.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# p.process_data()\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ProcessingData' is not defined"
     ]
    }
   ],
   "source": [
    "p=ProcessingData(data_structure_config)\n",
    "# p.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.read_csv(\"artifacts/data_ingestion/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>salmo.1558.jpg</td>\n",
       "      <td>Salmonella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cocci.1866.jpg</td>\n",
       "      <td>Coccidiosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cocci.171.jpg</td>\n",
       "      <td>Coccidiosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>salmo.1484.jpg</td>\n",
       "      <td>Salmonella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ncd.100.jpg</td>\n",
       "      <td>New Castle Disease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           images               label\n",
       "0  salmo.1558.jpg          Salmonella\n",
       "1  cocci.1866.jpg         Coccidiosis\n",
       "2   cocci.171.jpg         Coccidiosis\n",
       "3  salmo.1484.jpg          Salmonella\n",
       "4     ncd.100.jpg  New Castle Disease"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/1c/49/30ffcac5af06d08dfdd27da322ce31a373b733711bb272941877c1e4794a/scikit_learn-1.3.2-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\atanu\\anaconda3\\envs\\chicken\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\atanu\\anaconda3\\envs\\chicken\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\atanu\\anaconda3\\envs\\chicken\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.3 MB 320.0 kB/s eta 0:00:30\n",
      "   ---------------------------------------- 0.1/9.3 MB 1.0 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.3/9.3 MB 2.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.8/9.3 MB 4.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.0/9.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/9.3 MB 4.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/9.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/9.3 MB 4.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.3 MB 3.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.5/9.3 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.7/9.3 MB 3.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.9/9.3 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.1/9.3 MB 3.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.3/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.8/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.2/9.3 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.4/9.3 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.5/9.3 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.8/9.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.1/9.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.3/9.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.3/9.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.3/9.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.5/9.3 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.3 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.2/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.4/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.5/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.7/9.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.9/9.3 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.1/9.3 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.3/9.3 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.4/9.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.6/9.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.8/9.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.0/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.7/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.0/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.4/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 3.7 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
